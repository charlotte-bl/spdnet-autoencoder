# spdnet-autoencoder

## Introduction

### Goal

This project aims to implement an autoencoder designed for Symmetric Positive Definite (SPD) matrices. SPD matrices, such as covariance matrices, are commonly used to represent EEG data. These matrices have specific properties that we can leverage in our model to better represent and process them.

## Explication

The project consists of three main folders: data, models, and src.

/data: Contains the datasets generated by the program. By default, synthetic datasets are generated using `synthetic_data_block_diag.py`. If you create other datasets, they will be stored here. The filenames follow this naming convention: `synthetic-dataloader + method_of_generation + train/test/val + index`. Datasets with the same generation method and index are split into training, validation, and test sets randomly.

/models: Contains the models you create. It includes an example of the first matrix from the last batch for all three splits (train, test, val), saved as image files. The data and model weights are saved in `.pt` and `.pth` formats, respectively, for future use.

/results: Contains the results when you want to compare the performance of multiple models.

/src: Contains the source code of the program.

## Usage Instructions

This section provides instructions for how to use the program.

### Generating Data

Synthetic data is provided by default, but you can generate new data by running:

```bash
cd src/
python3 generate_synthetic_data.py
```

You can modify the default generation parameters in the function defined in `src/parsing.py`. To see the available options and parameters, run:

```bash
python3 generate_synthetic_data.py --help
```

#### Available options for data generation:
- `--synthetic_generation`: Specifies which generation method to use for the synthetic dataset. Available options are:
  - `block_diag` (default)
  - `geodesics`
  - `lambda_mu`
- `--number_dataset`: Number of datasets to generate (default: 1).
- `--batch_size`: The batch size for each dataset (default: 32).
- `--size_block_matrices`: Size of each block matrix (default: 8). This represents the size of each class of matrices in the dataset.
- `--noise`: Type of noise for denoising (choices: `none`, `gaussian`, `salt_pepper`, `masking`).
- `--std`: Standard deviation of noise (default: 1, applicable when noise is present).
- `--number_matrices`: Number of matrices per dataset (default: 300).

The generated data will be stored in the `/data/` directory.

### Train a Model

To train a model using the generated synthetic data, you can run the following command:

```bash
cd src/
python3 pipeline.py
```

Similar to data generation, the parameters for training can be adjusted in the `src/parsing.py` file. To view all available training options and parameters, run:

```bash
python3 pipeline.py --help
```

#### Available training options:
- `--epochs`: Number of epochs for training (default: 5).
- `--learning_rate`: Learning rate for training (default: 0.01).
- `--encoding_dim`: Encoding dimension of the autoencoder (default: 2).
- `--encoding_channel`: Encoding channel of the autoencoder (default: 1).
- `--loss`: Type of loss function. It can either be `riemannian` or `euclidean` (default: `riemannian`).
- `--layers_type`: Type of layers to be used. Available options are:
  - `regular`: Regular layers between input and output channels.
  - `by_halves`: Layers reduce dimensions by half until the layer size is less than 10x10.
  - `one_layer`: Model with only one layer.
  - `hourglass_channel`: Hourglass-style layers with varying channels.
- `--show`: Set this flag to visualize the training process.
- `--data`: Dataset to train and test the autoencoder with. It can be either `synthetic` or `bci` (default: `synthetic`).
- `--layers`: Number of layers in the model (relevant for `regular` layers_type).
- `--batch_size`: Size of the batch for training/validation/testing (default: 32).
- `--synthetic_generation`: Generation method for synthetic data (choices: `block_diag`, `geodesics`, `lambda_mu`).
- `--noise`: Type of noise for denoising (choices: `none`, `gaussian`, `salt_pepper`, `masking`).
- `--std`: Standard deviation of noise (default: 0.01).
- `--index`: Index for the synthetic dataset (default: 1).

**Important Notes:**
- The `--layers` parameter must be greater than 1 for the `regular` layer type. If set to `one_layer` or `by_halves`, the `--layers` parameter should not be specified.
- For `bci` data, do not specify `--index` or `--synthetic_generation`.
- When using `synthetic` data, do not specify `--batch_size`, `--noise`, or `--std`, as the synthetic data is already pre-generated.
- If `--noise` is set to `none`, do not specify `--std`.

**Examples:**
```bash
python3 pipeline.py --epochs 200 -m regular -o 4 -d 8 -l riemann --data bci -n gaussian --std 0.01 
```
will train a model for 200 epochs on bci data with gaussian noise with 0.01 as standard deviation with layers split regularly and in the code 4 channels of dimension 8x8.

```bash
python3 pipeline.py --epochs 150 -m one_layer -o 8 -d 4 -l euclid --data synthetic -t geodesics -i 1
```
will train a model for 150 epochs on synthetic data with euclidean loss generated along a geodesic with no noise, one layer, and in the code 8 channels of dimension 4x4. The data must me already generated.

### Evaluate a Model

To evaluate a trained model, run:

```bash
cd src/
python3 evaluate_one_model.py
```
This will show the performance (accuracy, trustworthiness, loss) according to the encoding dimension.
You still have to change the parameters you need in the code in evaluate_one_model.py .

### Compare Different Models

To compare the performance of different models, you can run the following:

```bash
cd src/
python3 compare_models.py
```

This will compare the models' performance on the same dataset and display the results.
You still have to change the parameters you need in the code in compare_models.py .

### Run a Pipeline

To test the influence of latent dimensions, run one of the provided pipeline script:

```bash
chmod +x run_pipeline_synthetic.sh
./run_pipeline_synthetic.sh
```

```bash
chmod +x run_pipeline_bci.sh
./run_pipeline_bci.sh
```

This will execute the pipeline to train multiple models with one command.
