\documentclass{beamer}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{svg}
\usepackage{cite}

\usetheme{Madrid}
\usecolortheme{default}

%------------------------------------------------------------
%This block of code defines the information to appear in the
%Title page
\title[AE SPDnet]
{Learning distributions on Riemannian manifolds}

\subtitle{Autoencoder for SPD matrices}

\author[CB]
{Charlotte BOUCHERIE, Florian YGER, Thibault DE SURREL}

\institute{LITIS}

\date[2025]
{2025}

%\logo{\includegraphics[height=1cm]{overleaf-logo}}

\AtBeginSection[]
{
  \begin{frame}
    \frametitle{Table of contents}
    \tableofcontents[currentsection]
  \end{frame}
}

%------------------------------------------------------------
% Start of document -- Autoencoder for SPD matrices
%------------------------------------------------------------
\begin{document}

\frame{\titlepage}

\begin{frame}
\frametitle{Table of contents}
\tableofcontents
\end{frame}

%---------------------------------------------------------
\section{Context}
%---
\subsection{Use of SPD matrices}
%---
\begin{frame}
\frametitle{Use of SPD matrices}
    \begin{itemize}
        \item Covariances matrices
        \item Used in computer vision, brain imaging, brain-computer interface (EEG)
    \end{itemize}
\end{frame}

%---
\subsection{Works on SPD matrices}
%---
\begin{frame}
\frametitle{A Riemannian Network for SPD Matrix Learning}
Introduction of a network architecture that preserves the properties of positive definite matrices for Deep Learning \cite{DBLP:journals/corr/HuangG16}
\begin{itemize}
    \item 3 different layers: BiMap, ReEig, LogEig
    \item We will base ourselves on these layers for our autoencoder    
\end{itemize}

\end{frame}
%-
\begin{frame}
    \frametitle{Different works}
    DreamNet: A Deep Riemannian Manifold Network for SPD Matrix Learning \cite{wang2022dreamnetdeepriemanniannetwork}
    \begin{itemize}
        \item Methodology for creating deep networks
        \item Stacked Riemannian Autoencoder (SRAE) at the end of the network
    \end{itemize}
    Riemannian Multinomial Logistics Regression for SPD Neural Networks \cite{chen2024riemannianmultinomiallogisticsregression}
    \begin{itemize}
        \item Adapting logistic regression for SPD matrices
        \item New specific layer for classification
        \item Use of Log-Euclidean Metric or Log-Cholesky Metric
    \end{itemize}
    SPD domain-specific batch normalization to crack interpretable unsupervised domain adaptation in EEG \cite{kobler2022spddomainspecificbatchnormalization}
    \begin{itemize}
        \item Specific batch normalization for SPD matrices
    \end{itemize}
\end{frame}
%-
\begin{frame}
    \frametitle{Different works}
    Riemannian batch normalization for SPD neural networks \cite{brooks2019riemannianbatchnormalizationspd}
    \begin{itemize}
        \item Specific batch normalization for SPD matrices
    \end{itemize}
    A Riemannian Residual Learning Mechanism for SPD Network \cite{10651149}
    \begin{itemize}
        \item Improves learning process for SPD networks
    \end{itemize}
    \frametitle{Different works}
    U-SPDNet: An SPD manifold learning-based neural network for visual classification \cite{WANG2023382}
    \begin{itemize}
        \item SPD matrices from visual data
    \end{itemize}
\end{frame}
%-
\begin{frame}
    \frametitle{Different works}
    Reducing the Dimensionality of SPD Matrices with Neural Networks in BCI \cite{articledimension}
    \begin{itemize}
        \item Simplification of complex data for a better interpretability and processing in BCI data
    \end{itemize}
    Schur's Positive Definite Network: Deep Learning in the SPD Cone With Structure \cite{pouliquen:hal-04726325}
    \begin{itemize}
        \item Shows that the use of the structure in the network improves the performances
    \end{itemize}
    Modeling Graphs Beyond Hyperbolic: Graph Neural Networks in Symmetric Positive Definite Matrices \cite{zhao2023modelinggraphshyperbolicgraph}
    \begin{itemize}
        \item Applies GNN to SPD matrices
        \item To model graph structures in SPD matrices
    \end{itemize}
\end{frame}
%-
\begin{frame}
    \frametitle{Different works}
    SymNet: A Simple Symmetric Positive Definite Manifold Deep Learning Method for Image Set Classification \cite{9390301}
    \begin{itemize}
        \item Image set classification
    \end{itemize}
    From Manifold to Manifold: Geometry-Aware Dimensionality Reduction for SPD Matrices \cite{harandi2014manifoldmanifoldgeometryawaredimensionality}
    \begin{itemize}
        \item Lower-dimensional and more discriminative SPD matrices from SPD matrices with orthonormal projection
    \end{itemize}
    Geometry-Aware Principal Component Analysis for Symmetric Positive Definite Matrices \cite{articlepcaaspd}
    \begin{itemize}
        \item PCA applied to SPD matrices
        \item Preserves more data variance
        \item Extends PCA from Euclidean to Riemannian geometries
    \end{itemize}
\end{frame}
%---
\subsection{Problems}
%---
\begin{frame}
    \frametitle{Existing solutions in Euclidean geometry}
    \begin{itemize}
    \item Flatten to tangent space
    \item Distance approximation: Frobenius distance
    $ L=||A-B||_F = \sqrt{\sum_{i,j}(A_{ij}-B_{ij})^2}$
    \end{itemize}
\end{frame}
%-
\begin{frame}
\frametitle{Problems in Euclidean geometry}
\begin{itemize}
    \item Does not preserve the curvature of space
    \item Non-optimal results with euclidean distance
    \item Swelling effect : the determinants of the interpolation of flattened matrices
    \end{itemize}
\end{frame}
%-
\begin{frame}
    \frametitle{Riemannian geometry}
    Riemann metric \textit{(AIRM)}: $\delta^2_R(X,Y)$ = $||\text{log}(X^{-1/2}YX^{-1/2})||^2_F$
    \begin{itemize}
        \item Measure the similarity between two SPD matrices while respecting the structure
        \item We will use it in our AE in the model, in the cost function and in the trustworthiness.
    \end{itemize}
    Representing information with SPD matrices has proven beneficial for many recognition tasks. 
    Considering Riemannian geometry comes at a high cost especially in high-dimensional ones that limits applicability.
\end{frame}
%-
%---
\subsection{Objectives}
%-
\begin{frame}
\frametitle{Objectives}
How to preserve the SPD matrix through the reconstitutions ?
\begin{itemize}
    \item Autoencoder for SPD matrices for dimension reduction
    \item Layer to do the reverse operations of the autoencoder
    \item Impact of loss for reconstruction error
\end{itemize}
\end{frame}
%---------------------------------------------------------
\section{Autoencoder}
\begin{frame}
    \frametitle{Autoencoder Basics}
    \begin{itemize}
        \item Unsupervised learning: measurement of reconstruction error
        \item Dimension reduction
        \item Learn the underlying patterns
        \item Used for generative models
\end{itemize}
\begin{center}
    $ \phi : \mathcal{X} \rightarrow \mathcal{F}$ , encoder \\
    $ \psi : \mathcal{F} \rightarrow \mathcal{X}$ , decoder \\
    $ \phi,\psi = \arg \min\limits_{\phi,\psi} || X-(\psi \circ \phi)X||^2$
\end{center}
\begin{center}
    \includesvg[width=0.6\textwidth]{figures/autoencoder.svg}
\end{center}

\end{frame}

%---
\subsection{Metrics}
%--
\begin{frame}
    \frametitle{Reconstruction error}
    \begin{itemize}
        \item For each matrix, we calculate the Riemannian distance with its reconstruction.
    \end{itemize}
    \begin{center}
        $ \phi : \mathbb{R}^{n_i \times n_i} \rightarrow \mathbb{R}^{n_o \times n_o}$ \\
        $ \psi : \mathbb{R}^{n_o \times n_o} \rightarrow \mathbb{R}^{n_i \times n_i}$ \\
        $ \phi,\psi = \arg\min\limits_{\phi,\psi} \delta^2_R(X,\psi (\phi(X))) =  \arg \min\limits_{\phi,\psi} ||\text{log}(X^{-1/2}\psi (\phi(X))X^{-1/2})||^2_F$
    \end{center}
    \end{frame}
    %-
\begin{frame}
\frametitle{Trustworthiness}
\begin{itemize}
    \item For each matrix, we take its k closest matrices in the output space and its closest matrices in the input space.
    \item The distance is the same used to calculate our cost function.
    \item We penalize proportionally to the difference in ranks in the input space.
    \item We do not penalize matrices coming closer together.
\end{itemize}
\begin{center}
    $ T(k) = 1 - \frac{2}{nk(2n-3k-1)} \sum\limits_{i=1}^n\sum\limits_{j\in\mathcal{N}_i^k} \max(0,(r(i,j)-k)) $
\end{center}
\end{frame}
%-
\begin{frame}
\frametitle{Accuracy}
\begin{itemize}
    \item We use MDM (Minimum Distance to Mean) to know the accuracy before reconstituting our matrices.
    \item For each class, a centroid is estimated according to our distance.
    \item We compare the initial accuracy with the final accuracy.
\end{itemize}
\end{frame}
%---
\subsection{Layers}
%--

\begin{frame}
\frametitle{BiMap layer}
\begin{itemize}
    \item The function of this layer is to generate more compact and more discriminative SPD matrices.
    \item Layer which performs a bilinear map $f_b$ to transform the initial matrices into new matrices of lower dimension.
\end{itemize}

\begin{center}
    $ X_k = f_b^{(k)}(X_{k-1};W_k)=W_kX_{k-1}W_k^T$
\end{center}

$W_k$ is of full rank to guarantee that $X_k$ remains SPD.

\begin{block}{Network parameters}
    Number of input filters/channels \textit{hi}, number of output filters/channels \textit{ho}, size of input matrix \textit{ni}, size of output matrix \textit{no}
\end{block}


\end{frame}
%--
\begin{frame}
\frametitle{ReEig layer}
\begin{itemize}
    \item The function of this layer is to improve discriminative performance by introducing nonlinearity, in the same way as ReLU.
    \item Introduction of a non-linear function $f_r$ which corrects the matrices by setting a threshold for low eigenvalues.
\end{itemize}

\begin{center}
    $ X_k = f_r^{(k)}(X_{k-1})=U_{k-1}\max(\epsilon I, \Sigma_{k-1})U_{k-1}^T$
\end{center}

\end{frame}

%--
\begin{frame}
\frametitle{LogEig/ExpEig layers}

\begin{block}{LogEig}
    The function of this layer is to be able to apply Riemann geometry to the output matrix.
\end{block}

\begin{center}
    $ X_k = f_l^{(k)}(X_{k-1})=\log (X_{k-1})=U_{k-1}\log(\Sigma_{k-1})U_{k-1}^T$
\end{center}

\begin{block}{ExpEig}
    The function of this layer is to apply the inverse function of the LogEig layer.
\end{block}

\begin{center}
    $ X_k = f_e^{(k)}(X_{k-1})=\exp (X_{k-1})$
\end{center}

\end{frame}
%---
\subsection{Models}
%--
\begin{frame}
\frametitle{One layer}
\begin{itemize}
    \item Single BiMap layer for the encoder from $ni\rightarrow no$ and $hi\rightarrow ho$.
    \item We look at the influence of the output dimension and the output layer.
    \item The decoder does the opposite operation.
\end{itemize}
\begin{center}
    {\tiny
    \includesvg[width=0.9\textwidth]{figures/one_layer.svg}
    }
\end{center}

\end{frame}

\begin{frame}
\frametitle{Two layers with funnel channels}
\begin{itemize}
    \item Two BiMap layers.
        \begin{itemize}
            \item $ni\rightarrow ni/2$ and $hi\rightarrow ho$.
            \item $ni/2\rightarrow no$ and $ho\rightarrow hi$.
        \end{itemize}
        \item We look at the influence of the number of intermediate channels and the output dimension.
        \item The decoder does the opposite operation.
\end{itemize}
\begin{center}
    {\tiny
    \includesvg[width=\textwidth]{figures/hourglass_channel.svg}
    }
\end{center}

\end{frame}


\begin{frame}
\frametitle{Multiple layers evenly distributed}
\begin{itemize}
    \item Number of BiMap layers set in parameters.
    \item Channels and intermediate matrix sizes based on the number of layers.
    \item The decoder does the opposite operation.
\end{itemize}
\begin{center}
    {\tiny
    \includesvg[width=\textwidth]{figures/regular.svg}
    }
\end{center}
\end{frame}

\begin{frame}
\frametitle{Multiple layers halved in dimension}
\begin{itemize}
    \item Number of BiMap layers and filters in layers depends on $ni$ and $no$.
    \item Matrix size divided by two, number of filters multiplied by two at each layer.
    \item The decoder does the opposite operation.
\end{itemize}
\begin{center}
    {\tiny
    \includesvg[width=\textwidth]{figures/by_halves.svg}
    }
\end{center}

\end{frame}
%---------------------------------------------------------

\section{Results}
%---

%---
\subsection{Synthetic data}
%--
\begin{frame}
\frametitle{Synthetic data}
\begin{itemize}
    \item We generate 
\end{itemize}
\end{frame}

%--
\begin{frame}
    \frametitle{Denoising autoencoder}
    
\end{frame}
%-
\begin{frame}
    \frametitle{Gaussian noise}
    
\end{frame}
%---
\subsection{BCI data}
%---

\begin{frame}
\frametitle{BCI data}

\end{frame}

%-
\begin{frame}
    \frametitle{Results}
    \begin{itemize}
        \item Prediction less accurate
        \item The more we preserve the neighborhood, the worse the accuracy becomes.
        \item Lossy compression
    \end{itemize}
\end{frame}
%-
\begin{frame}
    \frametitle{Ideas}
    \begin{itemize}
        \item Dataset with more complex datas 
        \item Influence of dropout layer/masking noise
    \end{itemize}
\end{frame}

%---------------------------------------------------------
\begin{frame}[allowframebreaks]
    \frametitle{References}
    \bibliographystyle{plain}
    \bibliography{bibtex}
\end{frame}


%---------------------------------------------------------
\end{document}